{"cells":[{"cell_type":"code","source":["# import locale\n","\n","# print(locale.getpreferredencoding())\n"],"metadata":{"id":"oNnp_OFjCmg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAGS4j8u0gdN","outputId":"85a45de9-03f2-494c-cb73-e027cfc0b40e","executionInfo":{"status":"ok","timestamp":1678377588617,"user_tz":-60,"elapsed":22991,"user":{"displayName":"Muiredach O'Riain","userId":"12544249359192333459"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-f5e874ed-d98a-caff-74d4-c25808413113)\n","svmem(total=89639665664, available=88039993344, percent=1.8, used=791904256, free=85528317952, active=484843520, inactive=3234336768, buffers=279719936, cached=3039723520, shared=1372160, slab=156463104)\n","Mounted at /content/gdrive\n"]}],"source":["!nvidia-smi -L\n","import psutil\n","print(psutil.virtual_memory())\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtK7KvgqCI-k"},"outputs":[],"source":["# Stopped mid level = 2? use continue\n","# Stopped mid level = 1 or 0? use upsample\n","\n","lemode = ''                        # 'ancestral','primed','continue','cutcontinue','upsample'\n","lemodel = ''                            # CUSTOM-MODEL-NAME or '1b_lyrics'\n","\n","leartist = \"unknown\"\n","legenre = \"unknown\"\n","\n","lecount = 3\n","lesample_length_in_seconds = 330\n","lesampling_temperature = .98616616\n","lehop = [1,1,0.0625]                        #default [.5,.5,.125], optimized [1,1,0.0625]\n","\n","lepath = '/content/gdrive/MyDrive/..'  #replace with path to output folder on Google Drive\n","\n","leprompt_length_in_seconds= 30\n","\n","lecut = 70                                  # used only on cutcontinue\n","\n","leexportlyrics = False\n","leprogress = False # leave false to prevent issue with time stuff that crashes the program\n","leautorename = True\n","\n","lemax_batch_size = 3\n","lelower_batch_size = 3\n","lechunk_size = 16\n","lelower_level_chunk_size = 32\n","\n","lemulti = False"]},{"cell_type":"markdown","metadata":{"id":"wQLsmqJFZJYR"},"source":["# Single"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nv1ZlwJ6ZI09"},"outputs":[],"source":["lelyrics = \"\"\" \"\"\"\n","leaudio_fileS = '/content/gdrive/MyDrive/...'   #if priming replace with path to the primer sample folder on Google Drive"]},{"cell_type":"markdown","metadata":{"id":"IY68XFXyZA95"},"source":["# Multi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p58mmzzXYoe"},"outputs":[],"source":["leaudio_fileM = ('/content/gdrive/MyDrive/....')    #0  if priming with multiple files replace with path to the primer sample folder on Google Drive\n","\n","lelyrics0 = \"\"\" \"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"H-8cvPn3CO4s"},"source":["# 1 😂 Sample 😂\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAGS4k1WCC_C","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31be8f00-bee3-4622-8561-84d14b8b7ef1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'jukebox-saveopt'...\n","remote: Enumerating objects: 1128, done.\u001b[K\n","remote: Counting objects: 100% (310/310), done.\u001b[K\n","remote: Compressing objects: 100% (40/40), done.\u001b[K\n","remote: Total 1128 (delta 275), reused 270 (delta 270), pack-reused 818\u001b[K\n","Receiving objects: 100% (1128/1128), 2.76 MiB | 16.73 MiB/s, done.\n","Resolving deltas: 100% (623/623), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./jukebox-saveopt\n"]}],"source":["if lemode=='ancestral':\n","  leprompt_length_in_seconds=None\n","  leaudio_file = None\n","###############################################################################\n","###############################################################################\n","\n","codes_file=None\n","\n","#!pip install git+https://github.com/openai/jukebox.git\n","\n","#@title Install Jukebox\n","#!pip install -q --upgrade git+https://github.com/craftmine1000/jukebox-saveopt.git\n","\n","!git clone https://github.com/craftmine1000/jukebox-saveopt.git\n","!sed -i 's/==/>=/g' jukebox-saveopt/requirements.txt\n","!pip install jukebox-saveopt/\n","\n","##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$#### autosave start\n","import os\n","from glob import glob\n","\n","filex = \"/usr/local/lib/python3.9/dist-packages/jukebox/sample.py\"\n","fin = open(filex, \"rt\")\n","data = fin.read()\n","fin.close()\n","\n","newtext = '''import fire\n","import os\n","from glob import glob\n","\n","from termcolor import colored\n","from datetime import datetime\n","\n","newtosample = True'''\n","data = data.replace('import fire',newtext)\n","\n","newtext = '''starts = get_starts(total_length, prior.n_ctx, hop_length)\n","        counterr = 0\n","        x = None\n","        for start in starts:'''\n","data = data.replace('for start in get_starts(total_length, prior.n_ctx, hop_length):',newtext)\n","\n","newtext = '''global newtosample\n","    newtosample = (new_tokens > 0)\n","    if new_tokens <= 0:'''\n","data = data.replace('if new_tokens <= 0:',newtext)\n","\n","newtext = '''counterr += 1\n","            datea = datetime.now()\n","            zs = sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)\n","            if newtosample and counterr < len(starts):\n","                del x; x = None; prior.cpu(); empty_cache()\n","                x = prior.decode(zs[level:], start_level=level, bs_chunks=zs[level].shape[0])\n","                logdir = f\"{hps.name}/level_{level}\"\n","                if not os.path.exists(logdir):\n","                    os.makedirs(logdir)\n","                t.save(dict(zs=zs, labels=labels, sampling_kwargs=sampling_kwargs, x=x), f\"{logdir}/data.pth.tar\")\n","                save_wav(logdir, x, hps.sr)\n","                del x; prior.cuda(); empty_cache(); x = None\n","            dateb = datetime.now()\n","            timex = ((dateb-datea).total_seconds()/60.0)*(len(starts)-counterr)\n","            print(f\"Step \" + colored(counterr,'blue') + \"/\" + colored( len(starts),'red') + \" ~ New to Sample: \" + str(newtosample) + \" ~ estimated remaining minutes: \" + (colored('???','yellow'), colored(timex,'magenta'))[counterr > 1 and newtosample])'''\n","data = data.replace('zs = sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)',newtext)\n","\n","\n","newtext = \"\"\"lepath=hps.name\n","        if level==2:\n","          for filex in glob(os.path.join(lepath + '/level_2','item_*.wav')):\n","            os.rename(filex,filex.replace('item_',lepath.split('/')[-1] + '-'))\n","        if level==1:\n","          for filex in glob(os.path.join(lepath + '/level_1','item_*.wav')):\n","            os.rename(filex,filex.replace('item_',lepath.split('/')[-1] + '-L1-'))\n","        if level==0:\n","          for filex in glob(os.path.join(lepath + '/level_0','item_*.wav')):\n","            os.rename(filex,filex.replace('item_',lepath.split('/')[-1] + '-L0-'))\n","        save_html(\"\"\"\n","if leautorename:\n","  data = data.replace('save_html(',newtext)\n","\n","if leexportlyrics == False:\n","  data = data.replace('if alignments is None','#if alignments is None')\n","  data = data.replace('alignments = get_alignment','#alignments = get_alignment')\n","  data = data.replace('save_html(','#save_html(')\n","\n","if leprogress == False:\n","  data = data.replace('print(f\"Step \" +','#print(f\"Step \" +')\n","\n","fin = open(filex, \"wt\")\n","fin.write(data)\n","fin.close()\n","##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$#### autosave end\n","\n","###CUSTOM MODEL############ To Sample with Google Drive\n","if not '1b' in lemodel and not '5b' in lemodel:\n","  lemodelpath = '/content/gdrive/MyDrive/The_Well_Trained_Algorithm_Step3/juke/' + lemodel + '_prior/checkpoint_latest.pth.tar'\n","  fin = open(\"/usr/local/lib/python3.9/dist-packages/jukebox/hparams.py\", \"rt\")\n","  data = fin.read();  fin.close()\n","\n","\n","  data += lemodel + \"\"\"_prior = Hyperparams()\n","\"\"\" + lemodel + \"\"\"_prior.update(prior_1b_lyrics)\n","\"\"\" + lemodel + \"\"\"_prior.restore_prior='\"\"\" + lemodelpath + \"\"\"'\n","\"\"\" + lemodel + \"\"\"_prior.level=2\n","HPARAMS_REGISTRY['\"\"\" + lemodel + \"\"\"_prior'] = \"\"\" + lemodel + \"\"\"_prior\n","\"\"\"\n","\n","\n","  #data = data.replace('y_bins=(10,100)','y_bins=(604, 7898)')\n","  data = data.replace('min_duration=60.0','min_duration=24.0')\n","  data = data.replace('max_duration=600.0','max_duration=666.0')\n","\n","  fin = open(\"/usr/local/lib/python3.9/dist-packages/jukebox/hparams.py\", \"wt\")\n","  fin.write(data);  fin.close()\n","\n","  fin = open(\"/usr/local/lib/python3.9/dist-packages/jukebox/make_models.py\", \"rt\")\n","  data = fin.read(); fin.close()\n","\n","  data = data.replace(\"#'your_model': \",\"'\" +lemodel + \"_model': \")\n","  data = data.replace('(\"you_vqvae_here\", \"your_upsampler_here\", ..., \"you_top_level_prior_here\")','(\"vqvae\",  \"upsampler_level_0\", \"upsampler_level_1\", \"' + lemodel + '_prior\"),')\n","\n","  data = data.replace(\"dist.barrier()\",\"dist.barrier(); print('1')\")\n","  data = data.replace(\"checkpoint = t.load(restore, map_location=t.device('cpu'))\",\"checkpoint = t.load(restore, map_location=t.device('cuda')); print('2')\")\n","\n","  fin = open(\"/usr/local/lib/python3.9/dist-packages/jukebox/make_models.py\", \"wt\")\n","  fin.write(data); fin.close()\n","\n","  lemodel = lemodel + \"_model\"\n","###CUSTOM MODEL END############\n","\n","import jukebox\n","import torch as t\n","import librosa\n","import os\n","\n","from datetime import datetime\n","\n","from IPython.display import Audio\n","from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n","from jukebox.hparams import Hyperparams, setup_hparams\n","from jukebox.sample import sample_single_window, _sample, \\\n","                           sample_partial_window, upsample, \\\n","                           load_prompts\n","from jukebox.utils.dist_utils import setup_dist_from_mpi\n","from jukebox.utils.torch_utils import empty_cache\n","rank, local_rank, device = setup_dist_from_mpi()\n","\n","print(datetime.now().strftime(\"%H:%M:%S\"))\n","\n","model = lemodel\n","hps = Hyperparams()\n","hps.sr = 44100\n","hps.n_samples = lecount\n","hps.name = lepath\n","\n","chunk_size = lechunk_size\n","max_batch_size = lemax_batch_size\n","\n","hps.levels = 3\n","hps.hop_fraction = lehop\n","\n","vqvae, *priors = MODELS[model]\n","vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 786432)), device)\n","top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)\n","\n","# Prime song creation using an arbitrary audio sample.\n","mode = lemode\n","codes_file=None\n","audio_file = (leaudio_fileS,leaudio_fileM)[lemulti==True]\n","prompt_length_in_seconds=leprompt_length_in_seconds\n","\n","\n","if os.path.exists(hps.name):\n","  # Identify the lowest level generated and continue from there.\n","  for level in [0, 1, 2]:\n","    data = f\"{hps.name}/level_{level}/data.pth.tar\"\n","    if os.path.isfile(data):\n","      mode = mode if 'continue' in mode else 'upsample'\n","      codes_file = data\n","      print(mode + 'ing from level ' + str(level))\n","      break\n","print('mode is now '+mode)\n","\n","sample_hps = Hyperparams(dict(mode=mode, codes_file=codes_file, audio_file=audio_file, prompt_length_in_seconds=prompt_length_in_seconds))\n","\n","sample_length_in_seconds = lesample_length_in_seconds\n","hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n","assert hps.sample_length >= top_prior.n_ctx*top_prior.raw_to_tokens, f'Please choose a larger sampling rate'\n","\n","if lemulti==True:\n","  metas = [dict(artist = leartist, genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics0, ), ]  + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics1, ), ]    + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics2, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics3, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics4, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics5, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics6, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics7, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics8, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics9, ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics10 ), ]   + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics11, ), ]  + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics12, ), ]  + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics13, ), ]  + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics14, ), ]  + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics15, ), ]  + [dict(artist = leartist,  genre = legenre, total_length = hps.sample_length, offset = 0, lyrics = lelyrics16, ), ]\n","else:\n","  metas = [dict(artist = leartist,\n","              genre = legenre,\n","              total_length = hps.sample_length,\n","              offset = 0,\n","              lyrics = lelyrics,\n","              ),\n","            ] * hps.n_samples\n","labels = [None, None, top_prior.labeller.get_batch_labels(metas, 'cuda')]\n","\n","\n","#----------------------------------------------------------2\n","\n","sampling_temperature = lesampling_temperature\n","lower_batch_size = lelower_batch_size\n","max_batch_size = lemax_batch_size\n","lower_level_chunk_size = lelower_level_chunk_size\n","chunk_size = lechunk_size\n","sampling_kwargs = [dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n","                        chunk_size=lower_level_chunk_size),\n","                    dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n","                         chunk_size=lower_level_chunk_size),\n","                    dict(temp=sampling_temperature, fp16=True,\n","                         max_batch_size=max_batch_size, chunk_size=chunk_size)]\n","\n","print(sample_hps.mode)\n","print(sample_hps.prompt_length_in_seconds)\n","print(hps.sr)\n","print(top_prior.raw_to_tokens)\n","print('aaaaaaaaaaaaaaaaaaaaaaaaaaaa 4.55')\n","\n","if sample_hps.mode == 'ancestral':\n","  zs = [t.zeros(hps.n_samples,0,dtype=t.long, device='cuda') for _ in range(len(priors))]\n","  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n","elif sample_hps.mode == 'upsample':\n","  assert sample_hps.codes_file is not None\n","  # Load codes.\n","  data = t.load(sample_hps.codes_file, map_location='cpu')\n","  zs = [z.cuda() for z in data['zs']]\n","  assert zs[-1].shape[0] == hps.n_samples, f\"Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}\"\n","  del data\n","  print('Falling through to the upsample step later in the notebook.')\n","elif sample_hps.mode == 'primed':\n","  assert sample_hps.audio_file is not None\n","  audio_files = sample_hps.audio_file.split(',')\n","  duration = (int(sample_hps.prompt_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n","\n","  x = load_prompts(audio_files, duration, hps)\n","  zs = top_prior.encode(x, start_level=0, end_level=len(priors), bs_chunks=x.shape[0])\n","  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n","elif sample_hps.mode == 'continue':\n","  data = t.load(sample_hps.codes_file, map_location='cpu')\n","  zs = [z.cuda() for z in data['zs']]\n","  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n","elif sample_hps.mode == 'cutcontinue':\n","  print('-------CUT INIT--------')\n","  lecutlen = (int(lecut*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n","  print(lecutlen)\n","  data = t.load(codes_file, map_location='cpu')\n","  zabaca = [z.cuda() for z in data['zs']]\n","  print(zabaca)\n","  assert zabaca[-1].shape[0] == hps.n_samples, f\"Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}\"\n","  priorsz = [top_prior] * 3\n","  top_raw_to_tokens = priorsz[-1].raw_to_tokens\n","  assert lecutlen % top_raw_to_tokens == 0, f\"Cut-off duration {lecutlen} not an exact multiple of top_raw_to_tokens\"\n","  assert lecutlen//top_raw_to_tokens <= zabaca[-1].shape[1], f\"Cut-off tokens {lecutlen//priorsz[-1].raw_to_tokens} longer than tokens {zs[-1].shape[1]} in saved codes\"\n","  zabaca = [z[:,:lecutlen//prior.raw_to_tokens] for z, prior in zip(zabaca, priorsz)]\n","  hps.sample_length = lecutlen\n","  print(zabaca)\n","  zs = _sample(zabaca, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n","  del data\n","  print('-------CUT OK--------')\n","  hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n","  data = t.load(sample_hps.codes_file, map_location='cpu')\n","  zibica = [z.cuda() for z in data['zs']]\n","  zubu = zibica[:]\n","  if transpose != [0,1,2]:\n","    zubu[2][0] = zibica[:][2][transpose[0]];zubu[2][1] = zibica[:][2][transpose[1]];zubu[2][2] = zibica[:][2][transpose[2]]\n","    zubu[1][0] = zibica[:][1][transpose[0]];zubu[1][1] = zibica[:][1][transpose[1]];zubu[1][2] = zibica[:][1][transpose[2]]\n","    zubu[0][0] = zibica[:][0][transpose[0]];zubu[0][1] = zibica[:][0][transpose[1]];zubu[0][2] = zibica[:][0][transpose[2]]\n","  zubu = _sample(zubu, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n","  print('-------CONTINUE AFTER CUT OK--------')\n","  zs = zubu\n","else:\n","  raise ValueError(f'Unknown sample mode {sample_hps.mode}.')\n","\n","\n","\n","print(datetime.now().strftime(\"%H:%M:%S\"))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZAGS4pnjbmI1"},"source":["# 2 🥳 Upsample 🥳\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAGS4LolpZ6w"},"outputs":[],"source":["!wget -O /root/.cache/jukebox/models/5b/prior_level_1.pth.tar https://openaipublic.azureedge.net/jukebox/models/5b/prior_level_1.pth.tar\n","!wget -O /root/.cache/jukebox/models/5b/prior_level_0.pth.tar https://openaipublic.azureedge.net/jukebox/models/5b/prior_level_0.pth.tar\n","\n","del top_prior\n","empty_cache()\n","top_prior=None\n","\n","import gc\n","gc.collect()\n","os.remove('/root/.cache/jukebox/models/5b/vqvae.pth.tar')\n","\n","\n","\n","print(datetime.now().strftime(\"%H:%M:%S\"))\n","del top_prior\n","empty_cache()\n","top_prior=None\n","\n","\n","upsamplers = [make_prior(setup_hparams(prior, dict()), vqvae, 'cpu') for prior in priors[:-1]]\n","labels[:2] = [prior.labeller.get_batch_labels(metas, 'cuda') for prior in upsamplers]\n","\n","zs = upsample(zs, labels, sampling_kwargs, [*upsamplers, top_prior], hps)\n","print(datetime.now().strftime(\"%H:%M:%S\"))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G2XtTS4tVqB8"},"source":["<font size=60> 🕴️‍♀️ </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1L4VJdt1wUP"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}